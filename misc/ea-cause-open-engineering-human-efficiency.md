Should "Improving Human Intellectual Efficiency" be a cause area?
=================================================================

I have an intuition about intellectual work: I think humans do too much work for what we get out of it. In ways that are entirely our own fault. Apologies in advance: this post is delivered as a personal perspective, as I do not have a higher vantage point available. Hence the title is a question; I'd like to know whether others agree or disagree and why.

So many things in our lives could be automated, yet we often haven't done it. And when we have done it, we've often done it in clumsy or inefficient ways. I'm not sure how best to explain this. But like, have you ever noticed how annoying it is to move data between your phone and PC? Or how you might have to buy a software package that uses a data format that isn't compatible with competing software? Or how you can't tell your Windows PC to treat the "audio out" as an "audio in" and record it (unless you are lucky enough to have an audio driver with that feature)? Or just how difficult it is to program computers in general?

Right now, millions of people depend on processing _structured data_ as _plain-text_ in something called "bash", via scripts that invoke a series of unintuitive GNU utilities that replicate the look-and-feel of an OS invented around 1970. If you're processing a file that contains a space or an apostrophe in its name, you'll be lucky if your script merely crashes instead of doing something completely bizarre, may god have mercy on your soul. Tree-structured data? That's really not what sed is for, guys. Bash still works well in some use cases, once you've invested the time to learn how to use it, but it pains me that millions of people spend years learning how to use this ancient software because it is somehow still occasionally the best tool. Why have we allowed software this clumsy to remain top dog for 50 years running?

As a software developer, I have been deeply frustrated with the software industry for 20 years now, because everything is just so much crappier than it could have obviously been. First example that comes to mind: in my current job we needed a Linear Solver. A C++ solver may have taken a lot of time to integrate in our C# codebase, and would complicate deployment, so I preferred a pure-C# solution and I figured somebody must have written a C# library for this. But no, not really. There was a Simplex solver, but it turned out to be One-Phase and we needed Two-Phase. So instead, I found a Two-Phase solver among Apache's Java libraries and (with a lot of effort) ported it to C#.

I was surprised: this library would be useful in a variety of different industries, and C# is a popular 21-year-old language. Why hadn't anyone open-sourced a library like this already? I rapidly found four sets of university lecture notes on how the algorithm works, and one toy C# implementation that looked like somebody's school project, but not a single usable implementation.

A better example: there is a process called "serialization" that is ubiquitous in computer software. Serialization (and its counterpart, deserialization) is a key behavior that apps use when saving files, loading files, or sending messages of any kind between computers.

The serialization features of C#, the language I use most often, have (IMO) always kind of sucked. At the last company I worked for, I developed a serialization engine that is better, in one way or another, than all the other C# serializers I've seen. Unlike many libraries, mine supports cyclic object graphs, multiple serialization formats, good performance, and best of all, easy-to-use and highly flexible customization of the serialization process.

After being laid off due to Covid-19, I wanted to keep working on this library, so I asked my former company if they could open-source it. They said no. It wasn't that there was some risk of the company harming itself by giving competitors access to technology, it was simply that the open-sourcing process (which they themselves chose) would take time and therefore cost money, so they wouldn't do it. I am, therefore, rewriting the library from scratch in my free time and will give it away for free.

This is a library that would be useful to _any_ C# developer, C# being a popular 21-year-old language.

So why hadn't anyone open-sourced a library like this earlier? I think it's two factors:

1. Incentives: corporations don't normally hire people to make general solutions applicable across thousands of industries, because most corporations care about just one or two industries. Even if they build something general, they don't open-source it, because there's no money in it. Companies want something commercial and something specific, leaving the "general" stuff to a handful of companies such as Microsoft. Microsoft, in turn, occasionally creates general solutions, but for reasons I don't understand, tends to narrow its scope to current fads (e.g. Microsoft was once dedicated to XML and SOAP, but now JSON and gRPC are the new fads, so Microsoft built independent libraries for all of these things and they have no resemblance to each other.)
2. To my own astonishment, I am one of the few software developers who is always interested in _universality_: I try to solve problems "once and for all" rather than inventing just another wheel, and I keep a close eye on interface design, as this is where the all-important interoperability happens. Few are willing to create and maintain open-source software without pay, and even fewer do so with a vision of universality in mind.

(Or, maybe somebody made this already and word is just not getting around.)

Bret Victor published a famous talk 9 years ago called [Inventing on Principle](https://www.youtube.com/watch?v=PUv66718DII), in which he demonstrated a grand vision for what software development should be like. Among the many people inspired by Bret's vision were Chris Granger and friends, who launched a company that built the Light Table IDE — and, if I understand correctly, ran out of money and abandoned the project about 5 years ago. No surprise there. Still today there are a few people plugging away at Bret-Victor-esque projects, but very little is finished or polished and nothing has gone mainstream. No surprise there.

It's hard to sell developer tools since the competition is free, and if you do succeed in charging money for them, no one will expect your product to go mainstream. Why? Well, I learned to code when I was 11 with BASIC, a free tool built into the computer's ROM. I didn't have money, no allowance and no support from my legal guardian (who didn't like my hobby and banned me from using the computer, so I did much of my programming at school). If it cost money I would not have used it, period. Children are the coders of the future, and many of them can't buy your product. And why would you want them to? Must everything turn a profit? The commercial route is exemplified by Mathematica: a wonderful product that has been around over 30 years, which is ignored by most hobbyists and professionals alike because the language and libraries are not free to use (or so I suppose).

My issue with the status quo isn't just that humans have to work much longer and harder than necessary to get things done, due to lousy tooling. It's also that it is impossible to keep up with the important technology developments. There is so much more to learn than can ever be learned — and I, for one, would rather spend my life learning _useful_ things instead of learning how to do the same task in Microsoft's new JSON library that I once did more easily in Microsoft's older XML library, or learning to do the same task in Python in a totally different way than I used to do it in C#. I am also concerned about the volume of historical baggage being created by the modern software industry, but this is little more than a vague unease.

In a few decades, somebody might invent a superintelligent AGI that may or may not be aligned. Humans cannot compete with such a thing on metrics like learning speed, or information storage capacity, or information recall ability, or planning. We may be inferior on pretty much every metric except "quantity of qualia".

What we can do, however, is **make human beings more capable** by reducing the amount of learning and effort required for humans to know things and do things. I dream of a world where you learn one or two programming languages and you can do pretty much everything with it. Where people who grew up in a poor household speaking Santali can, instead of struggling with English, learn a human interlanguage 10 times easier than English and use it to communicate reliably with English speakers via software assistance. Where a bachelor's degree is enough to read a scientific paper adjacent to your own field and actually understand the damn thing. Where more databases are freely available and data analysis tasks are easier.

The traditional EA analysis framework is "importance, tractability, neglectedness".

I don't know how to rate the importance of this as a cause area relative to others. Is it really so bad that millions of software developers spend their time on "plumbing" (which, it seems to me, mostly should have been a solved problem by now), and on "wheel reinventing" instead of building new things? Is it really so bad that people have to learn a language as difficult as English to participate in modern society? Is it really a problem that scientific research is so impenetrable — that the burden of comprehension always falls on the reader, not on the writer? That various databases of useful data about our world are proprietary or, if freely accessible, at least hard to use?

I am mostly a negative utilitarian at heart, so logically I should care most about reducing extreme poverty and things like that. And yet, I experience a lot of stress and unhappiness working under the status quo in my profession, as I eyeball the top of Lazlo's hierarchy of needs above me. My perspective is biased in the sense that most other developers don't seem to be nearly as bothered by the roadblocks in their path, and it would be selfish of me to suggest funding for a cause area just for my own benefit.

Still, I know you are not all negative utilitarians, so I ask you: isn't there a case to be made that improving human productivity — in ways that free markets alone don't accomplish — could have very positive second- and third-order benefits? I don't know how to estimate the EV of indirect benefits, so I hope someone else here will volunteer their skills to do a better analysis, with numbers.

The neglectedness is obvious. There is a little money in academia (whose main focus, of course, is publishing PDFs), and in open-source foundations (which mainly spend on things industry already depends on, not new things). But funding for "open engineering" or "public goods" projects is very scarce in general. Governments have a budget for "science" (though complaints abound about the amount of money available, the ways it is distributed, and the behaviors that are incentivized), but they rarely budget for any public goods such as open-source software, hardware, designs, educational materials, and other intellectual products.

What about tractability? Well, tractability will depend on the specific intervention considered.

I am not sure what other interventions are possible, but here's my thought: there should be an institute, akin to CEEALAR (formerly known as EA Hotel), that pays less than US$15/hr to "open engineers" who create public goods. If somebody wants to create and give away an intellectual product in certain categories of software, hardware, or educational materials (consistent with the mission of magnifying human ability), or wants to do research related to this, they can apply to the institute and normally get funded at levels far below their market value; at minimum, only room and board would be provided. (I would like it if a remote-work program were available too, mainly because interested parties cannot necessarily move to a country that has an institute. Oh, and is that pandemic thing still going on?)

The institute's role, aside from screening applicants, would be to provide camaraderie and guidance, to keep projects on track, and to help workers find partners interested in helping accomplish ambitious projects large enough to require or benefit from a team. The institute may benefit from one or more coaches, facilitators, cooks and/or groundskeepers (probably funded at market value, but here too, someone aligned with the cause might work for less). Real estate for the institute should be purchased opportunistically at a low price in an area with relatively low cost of living, but adequate infrastructure.

The purpose of low funding levels is threefold: first, to improve tractability by making the program more affordable. Second, to discourage people from applying unless they are truly dedicated (as, for example, I am dedicated to making better programming tools.) Third, because I believe it is good to err on the side of approving rather than denying applications. People with good ideas are sometimes unable, in the beginning, to articulate why their idea would be valuable in a way that is legible and convincing to others. So, if more projects can be approved by paying less for each, that's probably worthwhile. Why do I think so? Maybe it has something to do with the difficulty I have had justifying my own ideas sometimes, but I also observe that even very successful venture capitalists don't seem to have a great track record at picking winners. Presumably, picking the "best" public goods to fund is also difficult.

Of course, wealthy EAs directly funding an institute isn't the only way to go. Another route would be lobbying a government to spend on "open engineering" or "public goods". Alternately, feel free to read this piece as a call to fund more CEEALARs in different countries, since CEEALAR's activities are a superset of what I'm proposing.
